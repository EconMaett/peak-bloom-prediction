---
title: "Peak Bloom Prediction"
author: "econmaett"
date: "01/05/2025"
lang: en-US
format:
  html:
    embed-resources: true
fig-cap-location: bottom
tbl-cap-location: bottom
toc: true
editor: source
---

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, error = FALSE,
  message = FALSE,
  fig.align = "center",
  out.width = "80%"
)
```

```{r}
#| label: load-pkgs
#| output: false

invisible(Sys.setlocale("LC_ALL","English"))

library(tidyverse)
library(kableExtra)
library(rvest)
library(httr2)
library(jsonlite)
library(nasapower)
library(sf)
library(mapview)
```

## Instructions

In this analysis we demonstrate a simple way of predicting the peak bloom date over the next decade for all five locations required by the competition.

The models are simple in that they only use the past bloom dates observed at each location - no other covariates or additional information are considered.

At the end of this document ([Appendix A](#appendix-rnoaa)), we demonstrate a simple way to get historic temperature data for the four locations via the [NOAA web API](https://www.ncdc.noaa.gov/cdo-web/).

The original demo file is available on [GitHub](https://github.com/GMU-CherryBlossomCompetition/peak-bloom-prediction).

## Clean data

The data for the five sites is provided as a simple text file in CSV format.
Each file contains the dates of the peak bloom of the cherry trees at the respective site, alongside the geographical location of the site.

The six columns in each data file are:

- `location`: a human-readable location identifier (`string`).
- `lat`: (approximate) latitude of the cherry trees (`double`).
- `long`: (approximate) longitude of the cherry trees (`double`).
- `alt`: (approximate) altitude of the cherry trees (`double`).
- `year`: year of the observation (`integer`).
- `bloom_date`: date of peak bloom of the cherry trees ([ISO 8601](https://xkcd.com/1179/) date `string`). The "peak bloom date" may be defined differently for different sites
- `bloom_doy`: days since January 1st of the year until peak bloom (`integer`). January 1st corresponds to `1`.

### Kyoto

The file `kyoto.csv` has been obtained from Yasuyuki AONO (`aono(at)envi.osakafu-u.ac.jp`) at <http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/>.

The geographical location at `35.01198°N` (latitude), `135.6761°E` (longitude), `44` meters above sea levels (altitude) roughly corresponds to the **Nakanoshima** area of the **Arashiyama Park** in Kyoto, Japan.

The peak bloom date of the **Prunus jamasakura** is determined by a local news paper in Arashiyama (Kyoto, Japan).

Data prior to 1888 is extracted from various descriptions or estimated:

- Data from the 9th to the 14th centuries was acquired and analyzed by Aono and Saito (2010; International Journal of Biometeorology, 54, 211-219).

- Phenology for 15th to 21st centuries was acquired and analyzed by Aono and Kazui (2008; International Journal of Climatology, 28, 905-914).

Note that the data set for Kyoto includes implicit missing values (`NA`s), which I like make explicit so that we may spot the discontinuities in the time series plots.

[Chapter 18 "Missing Values"](https://r4ds.hadley.nz/missing-values) of Hadley Wickham's book R for Data Science (2e) describes different strategies for dealing with implicit `NA`s, such as pivoting the data to the wide format, using joins, or using the `{tidyr}` function `complete()`.

Let's load the data for Kyoto, Japan

```{r}
#| label: load-kyoto

kyoto <- read_csv("data/kyoto.csv") |> 
  complete(year = full_seq(year, 1), location) |> 
  fill(location, lat, long, alt, .direction = "downup")
```

Let's plot the data for Kyoto, Japan

```{r}
#| label: fig-kyoto
#| fig-cap: "Peak cherry blossom days in Kyoto, Japan"

fig_kyoto <- kyoto |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 0.5) +
  geom_point(color = "#ff0080", alpha = 0.5) +
  geom_smooth(method = "loess", formula = y ~ x, colour = "#644128", se = TRUE) +
  scale_x_continuous(breaks = c(seq(800, 2100, by = 100), NA)) +
  labs(title = "Peak cherry blossom days in Kyoto, Japan", x = NULL, y = NULL) +
  theme_light()

print(fig_kyoto)

ggsave(filename = "images/fig_kyoto.png", plot = fig_kyoto, bg = "white")
```

Here we have used **locally estimated scatterplot smoothing (LOESS)** or **locally weighted scatterplot smoothing (LOWESS)** to who how the average peak bloom day changed over the years. 

Note that we could explicitly model this with a **local level (+ trend)** state-space model that we fit with the **Kalman filter and Smoother**, which also allows us to effectively deal with **missing observations**.

MeteoSwiss suggests using a **Gaussian Kernel** with a *20-year window* to extract the trend, which is equivalent to using a **low-pass filter** that shuts down all higher frequencies.

It also looks like the observations exhibit **heteroskedasticity**, that is, the variance changes over time, and it looks like the variance has become more narrow over time.

We can use **changepoint detection** methods to test for the changing time trend and the changing variance.

Assuming that the data in the past is correct, there do seem to be seasonal cycles, perhaps connected to solar activity or the tilt of the earth.

There is a clear trend towards earlier blooming, perhaps connected to anthropogenic climate change and urbanization. Studies attribute about half of the earlier blooming times in recent years to the **urban heat island effect**, and the other half to **climate change**.

Modelling the local level and the trend would be useful if we are to forecast a couple years into the future, since an individual prediction may be incorrect, but on average, they may be good. 

However, since we are only interested in forecasting a single moment in time, which may be greatly influenced by local weather, we probably are better off using historical weather data that we augment with forecasts, either by professional forecasters or by extrapolating the previous predictors.

### Liestal

The file `liestal.csv` contains observations for **a single wild cherry (Prunus avium) tree** in **Liestal-Weideli**, at `7.4814°N` (latitude), `7.730519°E` (longitude) and `350` meters above sea levels (altitude). 

The **peak bloom date** is defined as the day when **25%** of the blossoms are in full bloom. The date is determined by [MeteoSwiss](https://www.meteoswiss.admin.ch/climate/climate-change/vegetation-development/long-term-series-of-phenological-observations.html).

This means that 70% of buds would be open a couple days later. This raises the question of whether we should model the locations separately.

The data is provided by **Landwirtschaftliches Zentrum Ebenrain, Sissach and MeteoSwiss**.

```{r}
#| label: load-liestal

liestal <- read_csv("data/liestal.csv")
```

Let's plot the data for Liestal, Switzerland:

```{r}
#| label: fig-liestal
#| fig-cap: "Peak cherry blossom days for Liestal, Switzerland"

fig_liestal <- liestal |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 1) +
  geom_point(color = "#ff0080", alpha = 0.5, size = 2.5) +
  geom_smooth(method = "loess", formula = y ~ x, colour = "#644128", se = TRUE) +
  scale_x_continuous(breaks = c(seq(1880, 2020, by = 20), NA)) +
  labs(
    title = "Peak cherry blossom days for Liestal, Switzerland", 
    x = NULL, y = NULL
  ) +
  theme_light()

print(fig_liestal)

ggsave(filename = "images/fig_liestal.png", plot = fig_liestal, bg = "white")
```

For Liestal you can see the impact of the infamous heat summer of 2003 which I remember from my childhood, although interestingly, there's another record in 1990.

### New York

The file `newyorkcity.csv` contains **only a single observation** for the year 2024. It concerns the **Yoshino Cherry (Prunus x yedoensis)** tree in **Washington Square Park**, New York City, at `40.7304 °N` (latitude), `-73.99809°E` (longitude) and `8.5` meters above sea level (altitude).

Let's load the data for New York City

```{r}
#| label: load-newyorkcity

newyorkcity <- read_csv("data/nyc.csv")
```

Luckily, there are some historical observations dating *back to 2019* in the data provided by **USA-NPN**.

The Washington Square Park has site id `32789` and the **Yoshino** cherry you should predict has species id `228`.

We have been provided with USA-NPN data on individual cherry trees by the competition organizers, and we can load it:

```{r}
#| label: load-historical-data-nyc

nyc_data_npn <- read_csv("data/USA-NPN_status_intensity_observations_data.csv") |> 
  filter(Site_ID == 32789, Species_ID == 228) |> 
  mutate(Observation_Date = mdy(Observation_Date))
```

This file now contains individual observations of the phenophase and not the actual peak bloom date.

For simplicity, we take the first day someone observed the flowers to be open as the peak bloom day.

This could be done in a more sophisticated way by also looking at the **reported intensity value**, recorded in the `Intensity_Value` column variable.

We could simply create an indicator variable that is `TRUE` if the variable `Phenophase_Status` is equal to `1`, and take the first `Day_of_Year` as our `bloom_doy`.

Alternatively, we may use the variable `Intensity_Value` which is either `-9999`, `95% or more`, `Less than 5%`, `25-49%`, `75-94%`, `50-74%`.

Since according to the variable `Individual_ID`, only one tree, `183140` is being observed, we can use the first date where `Intensity_Value` is `50-74%` as our `bloom_doy`.

```{r}
#| warning: true

nyc_data <- nyc_data_npn |>
  arrange(Observation_Date) |>
  mutate(year = year(Observation_Date)) |>
  group_by(year) |>
  summarize(
    first_flower_index = min(which((Intensity_Value %in% c("75-94%", "95% or more")))),
    bloom_date = as_date(strftime(Observation_Date[first_flower_index], format = "%Y-%m-%d")),
    bloom_doy = Day_of_Year[first_flower_index],
    .groups = "drop"
  ) |>
  filter(!is.na(bloom_doy)) |>
  select(-first_flower_index) |>
  mutate(location = "newyorkcity") |> 
  arrange(year)
```

Then we combine the data sets

```{r}
#| label: augment-nyc-data

newyorkcity <- newyorkcity |>
  bind_rows(nyc_data |> filter(year != 2024)) |>  # Can't add a year twice!
  fill(lat, long, alt)
```

We now have a total of four observations for New York City

```{r}
#| label: tbl-nyc-data
#| tbl-cap: "Augmented NYC observations"

newyorkcity |> 
  filter(location == "newyorkcity") |> 
  kable() |> 
  kable_styling()
```

### Vancouver

Let's load the data for Vancouver, BC

```{r}
#| label: load-vancouver

vancouver <- read_csv("data/vancouver.csv")
```

The file `vancouver.csv` contains observations for cherry trees of the cultivar  **Akebono** in **Maple Grove Park**, Vancouver, BC located approximately at `49.2236916°N` (latitude), `-123.1636251°E` (longitude), `24` meters above sea levels (altitude).

Unfortunately, there are **only three observations** for 2022, 2023, and 2024, respectively. But casual observations have been recorded in the form of **photos posted** to the [VCBF Neighbourhood Blog for Kerrisdale](https://forums.botanicalgarden.ubc.ca/threads/kerrisdale.36008/). You can **search the forum** for the keywords `"Akebono"` (i.e., the name of the cultivar) and `"Maple Grove Park"` (i.e., the location of the trees).

After searching through the web forum, I have decided to add the following observations to the data set:

```{r}
#| label: define-vancouver_forum

vancouver_forum <- tribble(
  ~ year, ~ bloom_date,
  2021, "2021-03-31", # Start: 2021-03-21, Full: 2021-03-31, Full: 2021-04-07
  2020, "2020-04-03", # Full bloom
  2018, "2018-03-18", # Full bloom
  2017, "2017-04-11", # Full bloom 
  2015, "2015-03-19", # Full bloom
  2011, "2011-04-01", # Full bloom was over on 2011-04-15
  2010, "2010-03-14", # 60% bloom on 2010-03-07
  2008, "2008-04-14" # Start: 2008-04-06, Full: 2008-04-14, finished: 2008-04-22
) |> 
  mutate(
    bloom_date = as_date(bloom_date),
    bloom_doy = as.numeric(1 + as_date(bloom_date) - as_date(paste0(year, "-01-01"))),
    location = "vancouver"
  )
```

We can do some additional plausibility checks. For example, spring 2010 was indeed unusually warm, [causing cherry blossoms to bloom during the Vancouver 2010 Winter Olympics](https://www.upi.com/News_Photos/view/upi/33393f9d20159ecb347de9aa1d670922/2010-Winter-Olympics-in-Vancouver/). National Geographic claimed it was the [warmest Olympics to date.](https://www.nationalgeographic.com/adventure/article/100212-vancouver-2010-warmest-winter-olympics).

We add these observations to the data set

```{r}
#| label: augment-vancouver

vancouver <- vancouver |> 
  bind_rows(vancouver_forum) |> 
  fill(lat, long, alt) |> 
  arrange(year)
```

Now we have a total of eleven observations for Vancouver, BC.

```{r}
#| label: tbl-augmented-vancouver-data
#| tbl-cap: "Augmented Vancouver data"

vancouver |> 
  filter(location == "vancouver") |> 
  kable() |> 
  kable_styling()
```

### Washington

The file `washingtondc.csv` has been obtained from the EPA's Climate Change Indicators in the United States <https://www.epa.gov/climate-indicators/cherry-blossoms>.

The geographical location at `38.88535°N` (latitude), `-77.03863°E` (longitude) and `0` meters above sea level (altitude) correspond to the location of the [Tidal Basin in Washington, DC](https://www.nps.gov/articles/dctidalbasin.htm).

The peak bloom date is defined as the day when **70%** of the **Yoshino Cherry (Prunus x yedoensis)** are in full bloom, as determined by the [National Park Service](https://www.nps.gov/subjects/cherryblossom/bloom-watch.htm).

Let's load the data for Washington DC

```{r}
#| label: load-washingtondc

washingtondc <- read_csv("data/washingtondc.csv")
```

Let's plot the data for Washington D.C:

```{r}
#| label: fig-washingtondc
#| fig-cap: "Peak cherry blossom days for Washington DC, USA"

fig_washingtondc <- washingtondc |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 1) +
  geom_point(color = "#ff0080", alpha = 0.5, size = 2.5) +
  geom_smooth(method = "loess", formula = y ~ x, colour = "#644128", se = TRUE) +
  scale_x_continuous(breaks = c(seq(1920, 2020, by = 20), NA)) +
  labs(title = "Peak cherry blossom days for Washington DC, USA", x = NULL, y = NULL) +
  theme_light()

print(fig_washingtondc)

ggsave(filename = "images/fig_washingtondc.png", plot = fig_washingtondc, bg = "white")
```

Now that we have loaded the datasets for the five locations, we can combine them into a single CSV file called `cherry.csv`.

```{r}
#| label: combine-cherry-csv
#| eval: false

cherry <- bind_rows(
  kyoto,
  liestal,
  newyorkcity,
  vancouver,
  washingtondc
)

write_csv(cherry, file = "data/cherry.csv")
```

Now we can load the `cherry.csv` file from our `data/` folder

```{r}
#| label: load-cherry

cherry <- read_csv("data/cherry.csv")
```

We can show the last three observation for each location

```{r}
#| label: tbl-01
#| tbl-cap: "latest 3 observations for each location"

cherry |> 
  group_by(location) |> 
  slice_tail(n = 3) |> 
  kable() |> 
  kable_styling()
```

We can visualize the time series for our five main locations next to each other. Notice the use of the `{stringr}` function `str_to_title()` to convert the first letter of the locations to uppercase.

```{r}
#| label: fig-cherries_1880
#| fig-width: 8
#| fig-height: 3
#| out-width: 100%
#| fig-cap: |
#|   Time series of peak bloom of cherry trees since 1880 at five different sites.

df_cherry_1880 <- cherry |> 
  filter(year >= 1880)

fig_cherries <- ggplot(data = df_cherry_1880, aes(year, bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab") +
  geom_point(color = "#ff0080", alpha = 0.5) +
  geom_smooth(method = "loess", formula = y ~ x, colour = "#644128", se = FALSE) +
  scale_x_continuous(breaks = c(seq(1880, 2020, by = 40), NA)) +
  facet_grid(cols = vars(str_to_title(location))) +
  labs(x = NULL, y = "Peak bloom (since Jan 1st)") +
  theme_light()

print(fig_cherries)

ggsave(filename = "images/fig_cherries.png", plot = fig_cherries, bg = "white")
```

We create summary statistics to get an overview over the data. Specifically, we are interested in the range of the `year` variable, the total number of observations at each location, and the mean, median, and standard deviation of the `bloom_doy` variable.

```{r}
#| label: tbl-summary-statistics
#| tbl-cap: "Summary statistics for the main data set"

cherry |> 
  filter(!is.na(bloom_doy)) |> 
  group_by(location) |> 
  summarise(
    min_year = min(year),
    max_year = max(year),
    n_obs = n(),
    mean = round(mean(bloom_doy)),
    median = round(median(bloom_doy)),
    sd = round(sd(bloom_doy))
  ) |> 
  kable() |> 
  kable_styling()
```

We see that the standard deviation for Liestal is much larger than the rest, where peak bloom dates are on average about a weak apart from their historical average.

We check for missing observations in the most recent years:

```{r}
cherry |> 
  filter(location == "kyoto") |> 
  filter(is.na(bloom_doy)) |> 
  tail() |> 
  kable() |> 
  kable_styling()
```

Kyoto has multiple implicit `NA`s, including in 1730, 1872, 1895, 1919, 1921, and 1945.

So if we use data since 1880, as the demo analysis suggests, we still have four `NA`s. For time series analysis, we usually assume equally-spaced observations, and therefore need to either cut the data to start in 1946, or to replace the `NA`s with a sensible value.

Note that the other four locations do not contain any implicit `NA`.

Let's plot the distribution of the peak bloom days for the locations:

```{r}
#| label: fig-histogram
#| fig-cap: "Histogram of peak bloom (since Jan 1st)"

cherry |> 
  filter(year >= 1950) |> 
  filter(!(location %in% c("newyorkcity", "vancouver"))) |> 
  ggplot(mapping = aes(x = bloom_doy, fill = location)) +
  geom_histogram() +
  labs(title = "Histogram of peak bloom (since Jan 1st)", x = NULL, y = NULL) +
  theme_light() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

Let's use a density plot instead:

```{r}
#| label: fig-density-plot
#| fig-cap: "Denisty plots of peak bloom (since Jan 1st)"

cherry |> 
  filter(year >= 1950) |> 
  filter(!(location %in% c("newyorkcity", "vancouver"))) |> 
  ggplot(mapping = aes(x = bloom_doy, fill = location, colour = location)) +
  geom_density(alpha = 0.5) +
  labs(title = "Denisty plots of peak bloom (since Jan 1st)", x = NULL, y = NULL) +
  theme_light() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

Let's plot all five time series in the same plot.

```{r}
#| label: fig-cherry-blossoms-combined
#| fig-cap: "Peak cherry blossom days for different locations"

fig_combined <- cherry |> 
  filter(year >= 1880) |> 
  ggplot(mapping = aes(x = year, y = bloom_doy, colour = location)) +
  geom_step(linetype = "dotted", lwd = 0.5) +
  geom_point(alpha = 0.5, size = 2.5) +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = c(seq(1880, 2020, by = 20), NA)) +
  labs(title = "Peak cherry blossom days by locations", x = NULL, y = NULL) +
  theme_light() +
  theme(legend.position = "bottom", legend.title = element_blank())

print(fig_combined)

ggsave(filename = "images/fig_combined.png", plot = fig_combined, bg = "white")
```

This graphic highlights the negative linear time trend for all locations.

It also highlights that a linear time trend will likely underestimate the accelerating trend towards earlier peak bloom dates.

A simple method such as **ETS**, which places a lower weight on observations far in the past, may be better at modelling the recent trend.

In any case, we should compare all our models against **benchmark models** such as the **naive model**, where the prediction is equal to the last observation, and a simple **AR(1)** model.

## Additional data

The competition organizers provide additional data sets for sites other than the main sites relevant for the competition. 

We can use these time series in your modeling to help with spatial and temporal extrapolation. 

For example, you can add predictors to these data sets and train models on additional sites.

### Japan

The data file `japan.csv` is provided by the [Japanese Meteorological Agency (JMA)](https://www.data.jma.go.jp/sakura/data/pdf/005.pdf) and contains recorded peak bloom dates for cherry trees at various sites across Japan.

The sample trees are located within a 5km radius of the location indicated in the data file. 

We can load the data set, make implicit missing values explicit and change the `location` variable to only include the city names in lowercase letters.

```{r}
#| label: read-japan

japan <- read_csv("data/japan.csv") |>  
  complete(year =  full_seq(year, 1), location) |> 
  mutate(location = str_remove(location, "Japan/")) |> 
  mutate(location = tolower(location)) |> 
  fill(location, lat, long, alt, .direction = "downup") |> 
  mutate(location = factor(location))
```

The `japan.csv` file contains observations between `r min(japan$year)` and `r max(japan$year)` for `r length(unique(japan$location))` locations in Japan.

Let's plot peak bloom dates for Kyoto, Japan

```{r}
#| label: fig-bloom-kyoto2
#| fig-cap: "Peak bloom dates in Kyoto, Japan"

fig_kyoto2 <- japan |>
  filter(location == "kyoto") |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 1) +
  geom_point(color = "#f72585", alpha = 0.5, size = 2.5) +
  geom_smooth(method = "loess", colour = "#644128", se = FALSE) +
  scale_x_continuous(breaks = c(seq(1950, 2020, by = 10), NA)) +
  labs(title = "Peak bloom (since Jan 1st) in Kyoto, Japan", x = NULL, y = NULL) +
  theme_light()

print(fig_kyoto2)

ggsave(filename = "images/fig_kyoto2.png", plot = fig_kyoto2, bg = "white")
```


### Korea

The file `south_korea.csv` contains **first flowering dates** of cherry trees at various sites across South Korea, curated by the **Korean Meteorological Administration (KMA)**.

Because these are **first flowering dates**, they are likely to occur earlier than hypothetical full bloom dates would be.

Trees in South Korea were assumed to be of the **Prunus Yedoensis** variety based on previous work (Hur, Ahn, and Shim 2014; Chung et al. 2009) and its frequent occurrence.

Again, we make implicit missing values explicit, and I change the location names to lowercase so that to match the style of the main data set.

```{r}
#| label: load-korea

korea <- read_csv("data/south_korea.csv") |> 
  complete(year =  full_seq(year, 1), location) |> 
  mutate(location = str_remove(location, "South Korea/")) |> 
  mutate(location = tolower(location)) |> 
  fill(location, lat, long, alt, .direction = "downup") |> 
  mutate(location = factor(location))
```

The `south_korea.csv` file contains observations between `r min(korea$year)` and `r max(korea$year)` at `r length(unique(korea$location))` locations in South Korea.

Let's plot the first bloom dates in Seoul, South Korea:

```{r}
#| label: fig-seoul
#| fig-cap: "First bloom in Seoul, South Korea"

fig_seoul <- korea |>
  filter(location == "seoul") |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 1) +
  geom_point(color = "#f72585", alpha = 0.5, size = 2.5) +
  geom_smooth(method = "loess", colour = "#644128", se = FALSE) +
  scale_x_continuous(breaks = c(seq(1980, 2020, by = 5), NA)) +
  labs(title = "First bloom (since Jan 1st) in Seoul, Korea", x = NULL, y = NULL) +
  theme_light()

print(fig_seoul)

ggsave(filename = "images/fig_seoul.png", plot = fig_seoul, bg = "white")
```

### MeteoSwiss

The data file `meteoswiss.csv` contains peak bloom dates for various sites across Switzerland, obtained from <https://opendata.swiss/en/dataset/phanologische-beobachtungen>.

Note that when you use the `{utils}` function `read.csv()` you need to specify the option `encoding = "UTF-8"` to allow for special characters that are common in the German language.

```{r}
#| label: load-meteoswiss

meteoswiss <- read_csv("data/meteoswiss.csv") |> 
  complete(year = full_seq(year, 1), location) |> 
  mutate(location = str_remove(location, "Switzerland/")) |> 
  mutate(location = tolower(location)) |> 
  fill(location, lat, long, alt, .direction = "downup") |> 
  mutate(location = factor(location))
```

The `meteoswiss.csv` file contains observations between `r min(meteoswiss$year)` and `r max(meteoswiss$year)` at `r length(unique(meteoswiss$location))` locations.

Plot the data for Zurich, Switzerland

```{r}
#| label: fig-meteoswiss
#| fig-cap: "Peak bloom (since Jan 1st) in Zurich, Switzerland"

fig_meteoswiss <- meteoswiss |> 
  filter(str_detect(location, "zürich-")) |>
  mutate(location = str_remove(location, "zürich-")) |> 
  ggplot(mapping = aes(x = year, y = bloom_doy)) +
  geom_step(linetype = "dotted", colour = "#ffa5ab", lwd = 1) +
  geom_point(color = "#f72585", alpha = 0.5, size = 2.5) +
  # geom_smooth(method = "loess", colour = "#644128", se = TRUE) +
  scale_x_continuous(breaks = c(seq(1950, 2020, by = 20), NA)) +
  labs(
    title = "Peak bloom (since Jan 1st) in Zurich, Switzerland",
    x = NULL, y = NULL
  ) +
  theme_bw() +
  facet_grid(cols = vars(str_to_title(location)))

print(fig_meteoswiss)

ggsave(filename = "images/fig_meteoswiss.png", plot = fig_meteoswiss, bg = "white")
```

### NPN

Two additional files are provided by the **USA National Phenology Network (USA-NPN)** and the many participants who contribute to its *Nature’s Notebook* program.

- `USA-NPN_individual_phenometrics_data.csv`: Individual Phenometrics. 2009--2021. Accessed at <http://doi.org/10.5066/F78S4N1V>.

```{r}
#| label: load-npn-individual-phenometrics

npn_ind_obs <- read_csv("data/USA-NPN_status_intensity_observations_data.csv") |> 
  mutate(Observation_Date = mdy(Observation_Date))

npn_ind_des <- read_csv("data/USA-NPN_individual_phenometrics_datafield_descriptions.csv")
```

- `USA-NPN_status_intensity_data.csv`: Status and Intensity. 2009--2021. USA-NPN, Tucson, Arizona, USA. Accessed at <http://doi.org/10.5066/F78S4N1V>.

```{r}
#| label: load-npn-status-intensity

npn_int_obs <- read_csv("data/USA-NPN_status_intensity_observations_data.csv")

npn_int_des <- read_csv("data/USA-NPN_status_intensity_datafield_descriptions.csv")
```

These can help augment the existing data sets for which you don't have many observations such as New York City. They also contain some useful **predictors**.

We ignore variables measured in degrees Fahrenheit (`Tmin_in_F`, `Tmax_in_F` and `AGDD_in_F`).

All weather variables are from **Daymet** (<http://daymet.ornl.gov>).

- `Observation_ID`: The unique identifier of each phenophase status record (subsequently referred to as "status record") in the database.

- `Update_Datetime`: The date and time the status record was last updated after original submission online. A value of `"-9999"` indicates the record has not been updated since this field was established in July 2014.

- `Site_ID`: The unique identifier of the site at which the status record was made. More information can be found in the ancillary data file for `"Site"`.

- `Latitude`, `Longitude` and `Elevation_in_Meters`: The latitude, longitude and elevation of the site at which the status record was made. Generally lat/long is calculated from the **Google Maps API** with a datum of `WGS84` (<https://developers.google.com/maps>). A value of `"-9999"` indicates the elevation could not be calculated.

- `State`: The U.S. state or territory, Mexican state or Canadian province in which the site is located. The state is calculated from lat/long by the **Google Maps Geocoding API** (<https://developers.google.com/maps/documentation/geocoding/intro>). A value of `"-9999"` indicates the site does not fall within the boundaries of North America.

- `Species_ID`: The unique identifier of the species for which the status record was made.

- `Genus`, `Species` and `Kingdom`: The taxonomic genus, species and kingdom of the organism for which the status record was made. Taxonomy follows that in the **Integrated Taxonomic Information System** (<http://itis.gov>). In those rare cases where a taxonomic subspecies or varietal is designated, the subspecies or varietal name is appended to the species name after a hyphen (e.g. *Cornus florida-appalachianspring*).

- `Common_Name`: The common name of the species for which the status record was made. Common names for plants follow those in the **USDA PLANTS Database** (<http://plants.usda.gov>), and for animals, in the **NatureServe database** (<http://explorer.natureserve.org>).

- `Individual_ID`: The unique identifier of the individual plant or the animal species at a site for which the status record was made. Note that for plants, individuals are tracked separately, while for animals, the species as a whole (rather than unique individuals) is tracked at a site. More information can be found in the ancillary data file for `"Individual Plant"`.

- `Phenophase_ID`: The unique identifier of the phenophase for which the status record was made. Note that there can be several names and definitions for a given phenophase as they vary between datasets and can change over time within datasets. All definitions lumped under the same `Phenophase_ID` are considered equivalent for the purposes of general phenological analysis. More information can be found in the ancillary data files for `"Phenophase"` and `"Phenophase Definition"`.

- `Phenophase_Description`: The descriptive title of the phenophase for which the status record was made. This is the overarching title of the phenophase defined by `Phenophase_ID`, and may include several different names and definitions (as defined in the `Phenophase_Name` and `Phenophase_Definition_ID` fields), as these vary between datasets and can change over time within datasets. More information can be found in the ancillary data file for `"Phenophase"`.

- `Observation_Date`: The date the status observation was made in the field.

- `Day_of_Year`: The day of year, ranging from `1` to `366`, that the status observation was made in the field.

- `Phenophase_Status`: The status (i.e. presence or absence) of the phenophase at the time the observation was made in the field.

- `Intensity_Category_ID`: The unique identifier of the intensity category assigned to the phenophase for this species at the time the status observation was made in the field. Intensity measures allow reporting of the degree to which a phenophase is expressed. More information can be found in the ancillary data file for `"Intensity"`. A value of `"-9999"` indicates there was no intensity category assigned to this phenophase for this species at the time the observation was made.

- `Intensity_Value`: The value reported in response to the intensity category question assigned to the phenophase for this species at the time the status observation was made in the field. The list of allowable responses for this intensity category question can be found in the ancillary data file for `"Intensity"`.

- `Abundance_Value`: Used only for animal observations, the value reported in response to the instruction, "enter the number of individual animals observed in this phenophase".

- `AGDD`: **Accumulated growing degree days (in degrees Celsius)** on the `Observation_Date`. This is the **sum of growing degree days (GDD) since January 1st**, where GDD is calculated as the number of degrees C by which each day's average temperature exceeds 0 degrees C (i.e., (Tmax + Tmin)/2 - 0 degrees C). From Daymet (<http://daymet.ornl.gov>).

- `Tmax_Winter`: Average maximum temperature (in degrees C) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

- `Tmax_Spring`: Average maximum temperature (in degrees C) for the spring season of the year of the Observation_Date (*March-May*). 

- `Tmax_Summer`: Average maximum temperature (in degrees C) for the summer season of the year of the Observation_Date (*June-August*).

- `Tmax_Fall`: Average maximum temperature (in degrees C) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date. 

- `Tmax`: Maximum temperature (in degrees C) on the Observation_Date.

- `Tmin_Winter`: Average minimum temperature (in degrees C) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

- `Tmin_Spring`:  Average minimum temperature (in degrees C) for the spring season of the year of the Observation_Date (*March-May*).

- `Tmin_Summer`: Average minimum temperature (in degrees C) for the summer season of the year of the Observation_Date (*June-August*).

- `Tmin_Fall`: Average minimum temperature (in degrees C) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date.

- `Tmin`: Minimum temperature (in degrees C) on the Observation_Date.

- `Prcp_Winter`: Accumulated precipitation (in mm) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

- `Prcp_Spring`: Accumulated precipitation (in mm) for the spring season of the year of the Observation_Date (*March-May*).

- `Prcp_Summer`: Accumulated precipitation (in mm) for the summer season of the year of the Observation_Date (*June-August*).

- `Prcp_Fall`: Accumulated precipitation (in mm) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date.

- `Prcp`: Precipitation (in mm) on the Observation_Date.

- `Accum_Prcp`: Accumulated precipitation (in mm) *from January 1st* to the Observation_Date.

- `Daylength`: Number of seconds of daylight on the Observation_Date.

## Maps

To explore the data, it can be helpful to create a simple map using the `{sf}` and `{mapview}` packages. 

We can use a third variable, such as the altitude or the number of observations to help our understanding of the dataset.

### Main locations

First, we plot the five locations in the `cherry.csv` file

```{r}
#| label: map-main-locations
#| eval: true

cherry_coord <- cherry |> 
  group_by(location) |> 
  slice_tail(n = 1)

cherry_map <- cherry_coord |> 
  st_as_sf(coords = c("long", "lat"), crs = 4326)

mapview(cherry_map, zcol = "alt")
```

### Japan

Next, we plot the map of all the locations in the `japan.csv` file

```{r}
#| label: map-japan
#| eval: true

coord_ch <- japan |> 
  filter(!is.na(bloom_doy)) |> 
  group_by(location, long, lat, alt) |> 
  summarise(n = n())

map_ch <- coord_ch |> 
  st_as_sf(coords = c("long", "lat"), crs = 4326)

mapview(map_ch, zcol = "n")
```


### Korea

Second, we plot the locations in the `south_korea.csv` file

```{r}
#| label: map-korea
#| eval: true

coord_korea <- korea |> 
  filter(!is.na(bloom_doy)) |> 
  group_by(location, long, lat, alt) |> 
  summarise(n = n())

map_korea <- coord_korea |> 
  st_as_sf(coords = c("long", "lat"), crs = 4326)

mapview(map_korea, zcol = "n")
```

### Switzerland

Third, we plot the locations in the `meteoswiss.csv` file

```{r}
#| label: map-meteoswiss
#| eval: true

coord_ch <- meteoswiss |> 
  filter(!is.na(bloom_doy)) |> 
  group_by(location, long, lat, alt) |> 
  summarise(n = n())

map_ch <- coord_ch |> 
  st_as_sf(coords = c("long", "lat"), crs = 4326)

mapview(map_ch, zcol = "n")
```

## Predictors

The goal of this section is to find additional publicly-available data that will improve your predictions.

### NOAA

For example, one source of global meteorological data comes from the **Global Historical Climatology Network (GHCN)**, available through the **NOAA web API**.

To use the web API, you first need a web service token.
You can request this token (free of charge) via <https://www.ncdc.noaa.gov/cdo-web/token>.

Once you have been issued the token, note it somewhere in your code (or make it available through an environment variable). Open your personal `.Renviron` file with the command `usethis::edit_r_environ()`. Note that you need to restart R after changing the `.Renviron` file. 

```{r}
#| echo: false

NOAA_WEB_API_TOKEN <- Sys.getenv("NOAA_WEB_API_TOKEN")
```

The stations closest to the sites for the competition **with continuously collected maximum temperatures** are `USC00186350` (Washington D.C.), `GME00127786` (Liestal), `JA000047759` (Kyoto), `CA001108395` (Vancouver) and 

```{r}
#| label: define-noaa-stations

NOAA_API_BASE_URL <- "https://www.ncei.noaa.gov/cdo-web/api/v2/data"

# Define the station IDs for the specified locations
stations <- c(
  "washingtondc" = "GHCND:USW00013743",
  "vancouver"    = "GHCND:CA001108395",
  "newyorkcity"  = "GHCND:USW00014732",
  "liestal"      = "GHCND:SZ000001940",
  "kyoto"        = "GHCND:JA000047759"
)
```

Note that the due to API changes, the `{rnoaa}` R package is can only retrieve data until 2022.

But the following functions allow you to use the new NOAA API directly.

You can explore ways to retrieve NOAA data here: <https://github.com/ropensci/rnoaa/issues/419>.

The `nested_to_tibble()` function will take a nested file obtained from the web and unlist it into a tibble object

```{r}
#| label: nested_to_tibble

nested_to_tibble <- function(x) {
  
  # Determine the variable names in the response
  variable_names <- map(x, names) |>
    unlist(use.names = FALSE) |>
    unique()

  names(variable_names) <- variable_names

  # Reshape the response from a nested list into a table
  map(variable_names, \(i) {
    map(x, \(y) {
      if (is.null(y[[i]])) {
        NA_character_
      } else {
        y[[i]]
      }
    }) |>
      unlist(use.names = FALSE)
  }) |>
    as_tibble()
}
```

The function `get_daily_avg_noaa()` will load data with specified data type IDs, such as `TMIN,TMAX,TAVG` from the dataset with the ID `GHCND`, the [Global Historical Climatology Network daily (GHCNd)](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily).

The new API is documented here: <https://www.ncei.noaa.gov/cdo-web/webservices/v2#data>.

The values that can be accessed are documented here: <https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt>.

The five core elements are:
- `PRCP`: Precipitation (tenths of mm)
- `SNOW`: Snowfall (mm)
- `SNWD`: Snow depth (mm)
- `TMAX`: Maximum temperature (tenths of degrees C)
- `TMIN`: Minimum temperature (tenths of degrees C)

Other intersting elements are
- `ASTP`: Average Station Level Pressure for the day (hPa * 10)
- `AWND`: Average daily wind speed (tenths of meters per second)

```{r}
#| label: get_daily_avg_noaa

get_daily_avg_noaa <- function(
    station_id, start_date, end_date,
    api_key, base_url, window_size = 300
    ) {
  
  windows <- seq(
    from = as_date(start_date),
    to = as_date(end_date) + days(window_size + 1),
    by = sprintf("%d days", window_size)
  )

  batches <- map2(windows[-length(windows)], windows[-1] - days(1), \(from, to) {
    
    if (from > Sys.Date()) {
      return(NULL)
    }
    
    response <- tryCatch(
      request(base_url) |>
        req_headers(token = api_key) |>
        req_url_query(
          datasetid = "GHCND",
          stationid = station_id,
          datatypeid = "PRCP,SNOW,SNWD,TMAX,TMIN,TAVG,ASTP,AWND",
          startdate = from,
          enddate = min(as_date(to), Sys.Date()),
          units = "metric",
          limit = 1000
        ) |>
        req_retry(max_tries = 10) |>
        req_perform() |>
        resp_body_json(),
      httr2_http = \(cnd) {
        rlang::warn(
          sprintf(
            "Failed to retrieve data for station %s in time window %s--%s",
            station_id, from, to
          ),
          parent = cnd
        )
        NULL
      }
    )
  })

  map(batches, \(x) nested_to_tibble(x$results)) |>
    list_rbind() |>
    mutate(date = as_date(date))
}
```

Next, we retrieve historic weather data from the NOAA API for the five locations with the functions `nested_to_tibble()` and `get_historic_noaa()`.

Because this function takes a long time to run, we instead load the file stored from a previous run. We can also [cache](https://quarto.org/docs/computations/caching.html#knitr-cache) this operation.

```{r}
#| label: load-historic-noaa
#| cache: true
#| eval: false

historic_noaa <- cherry |>
  group_by(location) |>
  summarize(start_date = sprintf("%d-01-01", pmax(1970, min(year)) - 1)) |>
  left_join(
    y = tibble(
      location = names(stations),
      station_id = stations
    ),
    by = "location"
  ) |>
  group_by(location) |>
  group_modify(\(x, gr) {
    get_daily_avg_noaa(
      station_id = x$station_id,
      start_date = x$start_date,
      end_date = Sys.Date(),
      api_key = NOAA_WEB_API_TOKEN,
      base_url = NOAA_API_BASE_URL
    )
  })
```

Save the NOAA data

```{r}
#| label: save-datasets
#| eval: false

write.csv(historic_noaa, file = "data/historic_noaa.csv", row.names = FALSE)
```

Read in the historical NOAA data. Remove the variables `station` and `attributes`.

```{r}
#| label: load-noaa

historic_noaa <- read_csv("data/historic_noaa.csv") |>
  select(-c("station", "attributes")) |> 
  mutate(date = as_date(date))
```

Let's plot the daily maximum temperatures. Be aware that there are again implicit missing values that you need to make explicit `NA`s first. Again, we use the `{tidyr}` function `complete()` in order to create a `date` observation for each combination of `location` and `datatype`. I have checked if this method ignores February 29 if there is an implicit missing value for that date by creating an explicit date vectors that includes leap years. Luckily, `complete()` seems to have a method implemented for objects of class `Date`. 

```{r}
#| label: complete-noaa

historic_noaa <- historic_noaa |> 
  complete(date =  full_seq(date, 1), location, datatype) |> 
  fill(location, .direction = "downup")
```

Note that the NCDC Climate Data Online (CDO) API limits each registered token to **five requests per second** and 10,000 requests per day.

As we will see, the NASA POWER API **does not have a set rate limit**. However, the NASA POWER Team monitors the usage and will limit usage to support equitable access.

Thus, for the NOAA CDO data, it's best to only load the whole data set once and then add single days. For NASA POWER, you can load the same data set repeatedly.

Let's plot `TMAX` with explicit `NA`s:

```{r}
#| label: fig-max-temp
#| fig-cap: "Average maximum temperature (°C)"

fig_tmax <- historic_noaa |>
  filter(datatype == "TMAX") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#d62828") +
  labs(
    title = "Average maximum temperature (°C)", 
    x = NULL, y = "Degrees (°C)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_tmax)

ggsave(filename = "images/fig_tmax.png", plot = fig_tmax, bg = "white")
```

Let's plot `TMIN` with explicit `NA`s:

```{r}
#| label: fig-min-temp
#| fig-cap: "Average minimum temperature (°C)"

fig_tmin <- historic_noaa |>
  filter(datatype == "TMIN") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#00b4d8") +
  labs(
    title = "Average minimum temperature (°C)",
    x = NULL, y = "Degrees (°C)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))


print(fig_tmin)

ggsave(filename = "images/fig_tmin.png", plot = fig_tmin, bg = "white")
```

Plot the `PRCP` with explicit `NA`s:

```{r}
#| label: fig-avg-prcp
#| fig-cap: "Average preciptation"

fig_prcp <- historic_noaa |>
  filter(datatype == "PRCP") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#4361ee") +
  labs(
    title = "Average preciptation (in mm)",
    x = NULL, y = "Average preciptation (in mm)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_prcp)

ggsave(filename = "images/fig_prcp.png", plot = fig_prcp, bg = "white")
```

Let's use the variable `PRCP` to calculate a new variable, `APRCP`, the accumulated precipitation (in mm) starting from January 1st of the year of the observation date. You need to decide what to do with `NA`s for `PRCP`. Either you set them equal to `0`, ignore them, or impute the values somehow.

Here, I create a temporary variable `PRCP2` which is equal to `0` if `PRCP` is `NA`. I then use the `{base}` function `cumsum()` to aggregate `APRCP`. In the end, I remove the temporary `PRCP2` variable. In the end I set all values for which `APRCP` is zero to `NA`.

```{r}
historic_noaa <- historic_noaa |> 
  pivot_wider(id_cols = c(location, date), names_from = datatype, values_from = value) |> 
  mutate(year = year(date)) |> 
  group_by(location, year) |> 
  arrange(location, year, date) |> 
  mutate(PRCP2 = if_else(is.na(PRCP), 0, PRCP)) |> 
  mutate(APRCP = cumsum(PRCP2)) |> 
  relocate(location, year, date, PRCP, PRCP2, APRCP) |> 
  ungroup() |> 
  select(-c(year, PRCP2)) |> 
  mutate(APRCP = if_else(APRCP == 0, NA, APRCP)) |> 
  pivot_longer(cols = -c(date, location), names_to = "datatype", values_to = "value")
```

Let's plot `APRCP`:

```{r}
fig_aprcp <- historic_noaa |>
  filter(datatype == "APRCP") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#4361ee") +
  labs(
    title = "Average aggregated preciptation (in mm)",
    x = NULL, y = "Average agregated preciptation (in mm)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_aprcp)

ggsave(filename = "images/fig_aprcp.png", plot = fig_aprcp, bg = "white")
```

Let's plot `SNOW` with explicit `NA`s:

```{r}
historic_noaa |>
  filter(datatype == "SNOW") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#03071e") +
  labs(x = NULL, y = "Average snowfall") +
  facet_grid(rows = vars(location))
```
Let's create the **Accumulated growing degree days (AGDD)** (in degrees Celsius) as the **sum of growing degree days (GDD) since January 1st**, where `GDD` is calculated as the number of degrees C by which each day's average temperature exceeds `0` degrees C (i.e., `GDD` = (`TMAX` + `TMIN`)/2 - `0` degrees C).

If an observation is missing either `TMIN` or `TMAX`, we cannot calculate the **midrange temperature** `TAXN` = (`TMAX` + `TMIN`) / 2, so we use the **average temperature** `TAVG` instead.

Note that we are using a **base temperature of zero degrees Celsius** instead of `4.3` or `4.4` degrees Celsius which is commonly assumed to be the base temperature for Yoshino cherry trees.

Here too, there is the question of what to do if `TAXN` is `NA`, which is possible if for an observation `TMIN`, `TMAX`, and `TAVG` are missing.

I create a temporary variable `GDD2`, which is set to zero if it is missing. Then I use `cumsum()` to sum over `GDD2` and get `AGDD`. In the end, I discard `GDD2` and all `AGDD` equal to zero.

```{r}
#| label: create-agdd

historic_noaa <- historic_noaa |> 
  pivot_wider(id_cols = c(date, location), names_from = datatype, values_from = value) |> 
  mutate(TAXN = (TMAX + TMIN) / 2) |> 
  mutate(TAXN = if_else(is.na(TAXN), TAVG, TAXN)) |>
  mutate(GDD = pmax(TAXN - 0, 0)) |>
  mutate(year = year(date)) |> 
  group_by(location, year) |> 
  arrange(location, year, date) |> 
  mutate(GDD2 = if_else(is.na(GDD), 0, GDD)) |> 
  mutate(AGDD = cumsum(GDD2)) |> 
  ungroup() |>
  select(-c(year, GDD2)) |> 
  mutate(AGDD = if_else(AGDD == 0, NA, AGDD)) |> 
  relocate(date, location, TMAX, TMIN, TAXN, TAVG, GDD, AGDD) |> 
  pivot_longer(cols = -c(date, location), names_to = "datatype", values_to = "value")
```

Let's plot the Growing Degree Days, `GDD`s with explicit `NA`s:

```{r}
#| label: fig-gdd
#| fig-cap: "Growing Degree Days (GDD) calculated from January 1st"

fig_gdd <- historic_noaa |>
  filter(datatype == "GDD") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#d62828") +
  labs(
    title = "Growing Degree Days (GDD)",
    subtitle = "Calculated from January 1st",
    x = NULL, y = "Growing Degree Days (GDD)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_gdd)

ggsave(filename = "images/fig_gdd.png", plot = fig_gdd, bg = "white")
```

Let's plot Accumulated Growing Degree Days, `AGDD`s with explicit `NA`s:

```{r}
#| label: fig-agdd
#| fig-cap: "Accumulated Growing Degree Days (AGDD) calculated from January 1s"

fig_agdd <- historic_noaa |>
  filter(datatype == "AGDD") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#d62828") +
  labs(
    title = "Accumulated Growing Degree Days (AGDD)", 
    subtitle = "Calculated from January 1st",
    x = NULL, y = "Accumulated Growing Degree Days (AGDD)"
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_agdd)

ggsave(filename = "images/fig_agdd.png", plot = fig_agdd, bg = "white")
```

Let's also calculate **Chilling Degree Days (CDD)** and **Accumulated Chilling Degree Days (ACDD)**. 

Common choices for the **start dates** are September 1st or October 1st, or even the first date when temperatures fall below a certain **threshold**, usually `7` degrees Celsius. 

We calculate these variables **until December 31st of each year**. 

I again need to introduce a temporary variable `CDD2` which is zero when values are missing. I save the new data types `CDD` and `ACDD` in a temporary file `historic_acdd` and add it to the `historic_noaa` file.

```{r}
#| label: create-acdd

historic_acdd <- historic_noaa |> 
  pivot_wider(id_cols = c(date, location), names_from = datatype, values_from = value) |> 
  mutate(year = year(date)) |> 
  filter(date >= paste0(year, "-10-01") & date <= paste0(year, "-12-31")) |> 
  mutate(CDD = pmax(0, 7 - TMIN)) |> 
  group_by(location, year) |> 
  arrange(location, year, date) |> 
  mutate(CDD2 = if_else(is.na(CDD), 0, CDD)) |> 
  mutate(ACDD = cumsum(CDD2)) |> 
  ungroup() |> 
  select(-c(year, CDD2)) |>
  mutate(ACDD = if_else(ACDD == 0, NA, ACDD)) |> 
  relocate(location, date, TMIN, CDD, ACDD) |> 
  pivot_longer(cols = -c(date, location), names_to = "datatype", values_to = "value") |> 
  filter(datatype == "ACDD" | datatype == "CDD")

historic_noaa <- bind_rows(historic_noaa, historic_acdd)
```

Note that we have to add explicit `NA`s for the implicitly missing `CDD` and `ACDD` values:

```{r}
#| label: complete-agdd

historic_noaa <- historic_noaa |> 
  complete(date =  full_seq(date, 1), location, datatype) |> 
  fill(location, .direction = "downup")
```


Let's plot the Chilling Degree Days, `CDD`s with explicit `NA`s:

```{r}
#| label: fig-plt-cdd
#| fig-cap: "Chilling Degree Days (CDD) calculated from October 1st"

fig_cdd <- historic_noaa |>
  filter(datatype == "CDD") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#00b4d8") +
  labs(
    title = "Chilling Degree Days (CDD)",
    subtitle = "Calculated from October 1st",
    x = NULL, y = "Chilling Degree Days",
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_cdd)

ggsave(filename = "images/fig_cdd.png", plot = fig_cdd, bg = "white")
```

Let's plot Accumulated Chilling Degree Days, `ACDD`s with explicit `NA`s:

```{r}
#| label: fig-plt-acdd
#| fig-cap: "Accumulated Chilling Degree Days (ACDD) calculated from October 1st"

fig_acdd <- historic_noaa |>
  filter(datatype == "ACDD") |>
  ggplot(aes(x = date, y = value)) +
  geom_line(colour = "#00b4d8") +
  labs(
    title = "Accumulated Chilling Degree Days (ACDD)",
    subtitle = "Calculated from October 1st",
    x = NULL, y = "Chilling Degree Days",
  ) +
  theme_minimal() +
  facet_grid(rows = vars(location))

print(fig_acdd)

ggsave(filename = "images/fig_acdd.png", plot = fig_acdd, bg = "white")
```

- `Tmax_Winter`: Average maximum temperature (in degrees C) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

- `Tmin_Winter`: Average minimum temperature (in degrees C) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

A simple model may simply take the average maximum winter temperature (Dec. 1st of the previous year until end of February) into account:

Let's create two additional variables: `TMAX_Winter` and `TMIN_Winter`, the average maximum and minimum temperature in degrees C for the winter season of the year of the observation date, from December 1st of the previous year to the end of February of the year of the observation date.

```{r}
historic_noaa

historic_noaa |>
  filter(datatype == "TMAX") |>
  mutate(year = case_when(
    month(date) < 3 ~ year(date),
    month(date) == 12 ~ year(date) + 1,
    TRUE ~ NA_integer_
  )) |>
  filter(!is.na(year), year >= 1970) |>
  group_by(location, year) |>
  summarize(
    TMAX_Winter = mean(value, na.rm = TRUE),
    .groups = "drop"
  )
```

Here are the next predictors we should create

- `Tmax_Spring`: Average maximum temperature (in degrees C) for the spring season of the year of the Observation_Date (*March-May*). 

- `Tmax_Summer`: Average maximum temperature (in degrees C) for the summer season of the year of the Observation_Date (*June-August*).

- `Tmax_Fall`: Average maximum temperature (in degrees C) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date. 

- `Tmin_Spring`:  Average minimum temperature (in degrees C) for the spring season of the year of the Observation_Date (*March-May*).

- `Tmin_Summer`: Average minimum temperature (in degrees C) for the summer season of the year of the Observation_Date (*June-August*).

- `Tmin_Fall`: Average minimum temperature (in degrees C) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date.

- `Prcp_Winter`: Accumulated precipitation (in mm) for the winter season of the year of the Observation_Date (*December of previous year to February of the year of the Observation_Date*).

- `Prcp_Spring`: Accumulated precipitation (in mm) for the spring season of the year of the Observation_Date (*March-May*).

- `Prcp_Summer`: Accumulated precipitation (in mm) for the summer season of the year of the Observation_Date (*June-August*).

- `Prcp_Fall`: Accumulated precipitation (in mm) for the previous year's fall season (*September-November*), relative to the year of the Observation_Date.

- `Daylength`: Number of seconds of daylight on the Observation_Date.

### POWER

We use the `{nasapower}` package described here <https://docs.ropensci.org/nasapower/> to access the [NASA Prediction Of Worldwide Energy Resources (POWER) API](https://power.larc.nasa.gov/).

We create a custom function called `get_historic_power()` to repeatedly query the API

```{r}
#| label: get_historic_power

get_historic_power <- function(
    var_climate,
    start_date,
    end_date,
    frequency_climate,
    long,
    lat
  ) {
  df_climate <- map2(
      .x = long,
      .y = lat,
      .f = function(long = .x, lat = .y) {
        res <- nasapower::get_power(
          community = "ag",
          lonlat = c(long, lat),
          pars = var_climate,
          dates = c(start_date, end_date),
          temporal_api = frequency_climate
        )
        return(res)
      }
    ) |>
    list_rbind()
  return(df_climate)
}
```


Time series with a daily resolution are available since `1981-01-01`.

We load the following variables:
- `TS`: Average earth surface temperature
- `TS_MAX`: Maximum temperature of the earth's surface
- `TS_MIN`: Minimum land surface temperature
- `EVLAND`: Evaporation from the earth's surface
- `FROST_DAYS`: Frost days
- `PRECTOTCORR`: Corrected precipitation
- `GWETPROF`: Soil moisture profile
- `RH2M`: Relative humidity at two meters
- `GWETROOT`: Soil moisture in the root zone
- `TO3`: The total amount of ozone in a column extending vertically from the Earth's surface to the top of the atmosphere

We define the variables we need in the vector `var_climate`

```{r}
#| label: define-var_climate

var_climate <- c(
  "TS",
  "TS_MAX",
  "TS_MIN",
  "EVLAND",
  "FROST_DAYS",
  "PRECTOTCORR",
  "GWETPROF",
  "RH2M",
  "GWETROOT",
  "TO3"
)
```

We also need to specify a `start_date`, an `end_date` and a `frequency`

```{r}
#| label: define-start-end-freq

start_date <- "1981-01-01"
end_date <- as.character(Sys.Date())
frequency_climate <- "daily"
```

We retrieve the **coordinates** which we will use to match to the nearest weather stations

```{r}
#| label: get-coordinates

coord_kyoto <- read_csv("data/kyoto.csv") |> 
  distinct(lat, long)

coord_liestal <- read_csv("data/liestal.csv") |> 
  distinct(lat, long)

coord_nyc <- read_csv("data/nyc.csv") |> 
  distinct(lat, long)

coord_vancouver <- read_csv("data/vancouver.csv") |> 
  distinct(lat, long)

coord_washingtondc <- read_csv("data/washingtondc.csv") |> 
  distinct(lat, long)

coord_meteoswiss <- read_csv("data/meteoswiss.csv") |>
  distinct(lat, long)

coord_korea <- read_csv("data/south_korea.csv") |>
  distinct(lat, long)

coord_japan <- read_csv("data/japan.csv") |>
  distinct(lat, long)

coord_npn1 <- read_csv("data/USA-NPN_individual_phenometrics_data.csv") |> 
  select(lat = Latitude, long = Longitude) |> 
  distinct_all()

coord_npn2 <- read_csv("data/USA-NPN_status_intensity_observations_data.csv") |> 
  select(lat = Latitude, long = Longitude) |> 
  distinct_all()

coord_npn <- bind_rows(coord_npn1, coord_npn2) |> 
  distinct_all()
```

Load NASA POWER data for Kyoto, Japan

```{r}
#| label: load-nasa-kyoto

df_climate_kyoto <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_kyoto$long,
  lat = coord_kyoto$lat
  )
```

Load NASA POWER data for Liestal, Switzerland

```{r}
#| label: load-nasa-liestal

df_climate_liestal <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_kyoto$long,
  lat = coord_kyoto$lat
  )
```

Load NASA POWER data for New York City, USA:

```{r}
#| label: load-nasa-nyc

df_climate_nyc <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_nyc$long,
  lat = coord_nyc$lat
  )
```

Load NASA POWER data for Vancouver, Canada

```{r}
#| label: load-nasa-vancouver

df_climate_vancouver <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_vancouver$long,
  lat = coord_vancouver$lat
  )
```

Load NASA POWER data for Washington DC, USA:

```{r}
#| label: load-nasa-washingtondc

df_climate_washingtondc <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_washingtondc$long,
  lat = coord_washingtondc$lat
  )
```

Combine the data for the five main locations and save it to a single file called `df_climate_cherry.csv`

```{r}
#| label: combine-nasa-datasets

df_climate_cherry <- bind_rows(
  df_climate_kyoto,
  df_climate_liestal,
  df_climate_nyc,
  df_climate_vancouver,
  df_climate_washingtondc
)

write_csv(df_climate_cherry, file = "data/df_climate_cherry.csv")
```


We also load predictors for the `japan.csv` file. Note that this takes a long time to run, so we load the data from a previous run instead.

```{r}
#| label: load-nasa-japan
#| eval: false

df_climate_japan <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_japan$long,
  lat = coord_japan$lat
  )

write_csv(df_climate_japan, file = "data/df_climate_japan.csv")
```

We also load predictors for the `south_korea.csv` file. Again, we use data from a previous run because this operation does take som etime.

```{r}
#| label: load-nasa-korea
#| eval: false

df_climate_korea <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_korea$long,
  lat = coord_korea$lat
  )

write_csv(df_climate_korea, file = "data/df_climate_korea.csv")
```

We also load predictors for the `meteoswiss.csv` file that we have previously downloaded.

```{r}
#| label: load-nasa-meteoswiss
#| eval: false

df_climate_meteoswiss <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_meteoswiss$long,
  lat = coord_meteoswiss$lat
  )

write_csv(df_climate_meteoswiss, file = "data/df_climate_meteoswiss.csv")
```

Finally, we load predictors for the USA-NPN CSV files from a previous run

```{r}
#| label: load-nasa-npn
#| eval: false

df_climate_npn <- get_historic_power(
  var_climate = var_climate,
  start_date = start_date,
  end_date = end_date,
  frequency_climate = frequency_climate,
  long = coord_npn$long,
  lat = coord_npn$lat
  )

write_csv(df_climate_npn, file = "data/df_climate_npn.csv")
```

Let's have a look at the NASA POWER dataset:

```{r}
# df_climate_cherry 
```

### AccuWeather

Any weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.

We use the AccuWeather forecast as it appeared on the webpage [AccuWeather.com](https://www.accuweather.com/en/be/brussels/27581/january-weather/27581?year=2023) on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the [Internet Archive](https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/january-weather/27581?year=2023&unit=c). The following R code creates the function `get_weather_table()` to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The `get_weather_table()` function combines several functions from the `{rvest}` package, which is yet another member of the `{tidyverse}`. In particular, the forecast on each page is contained within the div `monthly-calendar` and can be extracted with the `html_nodes()` and `html_text2()` functions.

Applying the `get_weather_table()` function to the url for each page yields a five column tibble `temp_br`, with columns defined in the same way as the tibble `temp`, discussed in previous sections. The first 10 rows are below; the data are also available on the author's [GitHub](https://raw.githubusercontent.com/jauerbach/miscellaneous/main/temp_br.csv).

We now predict the day the lilacs will bloom. The R code below uses the `doy_prediction` and `doy_last_frost` functions created in earlier sections and displays the prediction in a table. 

At the time of our writing, the predicted date is **April 19**. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url [https://web.archive.org/save](https://web.archive.org/save) to save a webpage to the Internet Archive to ensure your work is reproducible.)

## Prediction

A simple method to predict peak bloom date in the future is to fit a least-squares line through the observed dates and extrapolate the regression function.

We want to have a separate line for each location, hence we tell R to estimate *interaction* effects.

We only use data **from 1880** to fit the trends, as prior data may not be as reliable/relevant.

Since for New York City we only have one observed bloom date, which is not enough to fit a linear regression model, we will omit the site from this simple analysis.

Thus we fit a model **without intercept** and **four dummy variables** for every location except New York City, and we create interaction effects, meaning that in essence we fit four different models, one for each location, where each location has a different intercept and a different slope.

This is similar to fitting a linear time trend in time series analysis. 

```{r}
# Remove New York City which has only one observation
cherry_no_nyc <- cherry |>
  filter(location != "newyorkcity")

# Fit a linear time trend without intercept, dummy variables and interaction effect
ls_fit <- lm(
  formula = bloom_doy ~ 0 + location + location:year,
  data = cherry_no_nyc, 
  subset = year >= 1880
)
```

This simple linear regression functions suggest a trend toward earlier peak bloom at the sites.

```{r}
summary(ls_fit)
# modelsummary(ls_fit, output = "tinytable")
```

Unsurprisingly, neither the intercept nor the time trend are statistically significant for Vancouver where we have only three observations.

Note that we could have fitted the four models separately.

R automatically creates the interaction variables, that is, the four different intercepts and the four different slopes.

However, depending on the sample size, the time trend may change, as there is a structural break in the data, with the slope becoming much steeper over the past years, perhaps due to the effect of anthropogenic climate change and urbanization.


We can compute the actual predictions using the `predict()` function and 

```{r}
# Compute the predictions for all 4 sites

# Create all combinations for the years and locations as input variables, the past years and the current years
cherry_no_nyc_grid <- expand_grid(
  location = unique(cherry_no_nyc$location),
  year = 1880:2025
)

# Create point estimates and 90% confidence intervals for the input variables, which includes the current year
predictions <- cherry_no_nyc_grid |>
  bind_cols(predict(
    object = ls_fit,
    newdata = cherry_no_nyc_grid,
    interval = "prediction", level = 0.9
  )) |>
  rename(prediction = fit, lower = lwr, upper = upr)
```

Plot the predictions for the actual observations since 2015, i.e., until 2024:

```{r}
#| label: fig-predictions1
#| fig-width: 8
#| fig-height: 3
#| out-width: 100%
#| fig-cap: |
#|   Predictions and 90% prediction intervals from simple linear regression models fitted to
#|   four sites.

cherry_no_nyc |>
  right_join(predictions, by = c("year", "location")) |>
  filter((location == "vancouver" & year >= 2021) | (location != "vancouver" & year >= 2015)) |>
  ggplot(aes(x = year, y = prediction, ymin = lower, ymax = upper)) +
  geom_line(linewidth = 1) +
  geom_ribbon(color = "black", linetype = "22", linewidth = 0.8, fill = NA) +
  geom_point(aes(y = bloom_doy)) +
  scale_x_continuous(breaks = c(2015, 2018, 2021, 2024)) +
  facet_grid(cols = vars(str_to_title(location))) +
  labs(x = NULL, y = "Peak bloom (days since Jan 1st)")
```

Note that we had to remove all predictions for Vancouver for the years before 2021, when we have actual data for this location.

We see that the slope of the time trend has become steeper since 2015. Since then, peak bloom has been reached on average almost two days earlier in Vancouver and Liestal. If we assume that this trend continues this year, then it is clear that our estimate will be too high, as can be seen in the solid line lying above the actual observations in recent years.

We create a function that converts the day of year number into an ISO date:

```{r}
#' Small helper function to convert the day of year to the actual date.
#'
#' @param year year as an integer
#' @param doy day of the year as integer (1 means January 1st)
#' @return date string

doy_to_date <- function(year, doy) {
  strptime(paste(year, doy, sep = "-"), "%Y-%j") |> # create date object
    strftime("%Y-%m-%d") # translate back to date string in ISO 8601 format
}
```

Since we treated the outcome variable as continuous, we need to use `floor()` and `ceiling()` to create upper and lower bounds for our point estimate, for which we use `round()`.

Based on this very simple model, the peak bloom dates at these sites in 2025 are predicted to occur on:

**NOTE**: Add appropriate column names to this table.

```{r}
#| label: tbl-02
#| tbl-cap: "Predicted peak bloom dates at the four sites"

predictions |>
  filter(year == 2025) |>
  mutate(
    prediction = round(prediction),
    lower = floor(lower),
    upper = ceiling(upper),
    prediction_date = doy_to_date(year, prediction)
  ) |> 
  kable() |> 
  kable_styling()
```

Note that we would have predicted much earlier peak bloom times had we estimated the linear time trend on a much more recent sample of observations.

### Vancouver

We need to *extrapolate* from what we have learned about the peak bloom dates in the other locations to Vancouver and NYC.

The simple model we have fitted above, however, does not allow us to transfer any knowledge from the other sites -- we have only used the history trend at the respective sites.

Although the climate in Vancouver and NYC is different from the other locations, the simplest way to borrow information from the other locations is to average across these three sites.

Hence, we want to fit a straight line through the peak bloom dates, ignoring the actual site.

That is, we fit a simple linear time trend on all available observations, but we want to use larger weights for observations from Vancouver than for the other sites.

The `weights` argument must be a vector of the same length as the input data and we can use the names of the `location` column to create it. "Weighted Least Squares" or WLS will be used in the estimation.

We could have created a new variable explicitly with:

```{r}
#| eval: false

cherry_no_nyc$weight <- (cherry_no_nyc$location == "vancouver") + 0.2 * (cherry_no_nyc$location != "vancouver")

cherry_no_nyc$weight <- NULL
```

So the weight is one for the three observations that are actually from Vancouver and weighted with one fifth for all other locations.

```{r}
# Fit simple least-squares lines for all sites.
# We use larger weights for observations from Vancouver than for the other sites

ls_fit_for_van <- lm(
  formula = bloom_doy ~ year,
  data = cherry_no_nyc, subset = year >= 1880,
  weights = (location == "vancouver") + 0.2 * (location != "vancouver")
)

# Add the three observations plus the inputs for the prediction of the current year
vancouver_grid <- tibble(location = "vancouver", year = 2008:2025)

predictions_vancouver <- vancouver_grid |>
  bind_cols(predict(
    object = ls_fit_for_van,
    newdata = vancouver_grid,
    interval = "prediction", level = 0.9
  )) |>
  rename(prediction = fit, lower = lwr, upper = upr) |> 
  mutate(
    prediction = round(prediction),
    lower = floor(lower),
    upper = ceiling(upper),
    prediction_date = doy_to_date(year, prediction)
  )
```

Not surprisingly, the predicted peak bloom date for Vancouver and NYC is now very similar to the other 3 sites:

**NOTE**: Add appropriate column names to this table.

```{r}
#| label: tbl-03
#| tbl-cap: "Predictions for Vancouver"

predictions_vancouver |> 
  kable() |> 
  kable_styling()
```

Due to **rounding** and the **slope** being not very flat, the predictions are almost the same for every year.

We can check the predictions against the data from previous competition years.

```{r}
#| label: fig-predictions3
#| fig-width: 8
#| fig-height: 3
#| out-width: 100%
#| fig-cap: |
#|   Predictions and 90% prediction intervals from a simple linear regression model for
#|   Vancouver using data from all four sites.

cherry_no_nyc |>
  right_join(y = predictions_vancouver, by = c("year", "location")) |>
  ggplot(aes(x = year, y = prediction, ymin = lower, ymax = upper)) +
  geom_line(linewidth = 1) +
  geom_ribbon(color = "black", linetype = "22", linewidth = 0.8, fill = NA) +
  geom_point(aes(y = bloom_doy)) +
  scale_x_continuous(breaks = 2008:2024) +
  facet_grid(cols = vars(str_to_title(location))) +
  labs(x = NULL, y = "Peak bloom (days since Jan 1st)")
```
We are still quite far off.

We have made this very cautious prediction since we don't know much about the conditions in Vancouver.

If satisfied with the predictions, we can use them instead of the predictions from before.

```{r}
predictions <- predictions |>
  filter(location != "vancouver") |>
  bind_rows(predictions_vancouver)
```

### New York

Using the same steps as for Vancouver, BC, a simple linear model can be fitted for New York City.

We again combine predictions from the whole data set, weighting observations for New York City higher than that for other locations.

```{r}
#| label: tbl-04
#| tbl-cap: "Predictions for New York City"

# Fit simple least-squares lines for all sites.
# We use larger weights for observations from NYC than for the other sites

ls_fit_for_nyc <- lm(
  formula = bloom_doy ~ year,
  data = cherry, subset = year >= 1880,
  weights = (location == "newyorkcity") + 0.2 * (location != "newyorkcity")
)

# Add inputs since 2019 and the current year
nyc_grid <- tibble(location = "newyorkcity", year = 2019:2025)

predictions_nyc <- nyc_grid |>
  bind_cols(predict(
    object = ls_fit_for_nyc,
    newdata = nyc_grid,
    interval = "prediction", level = 0.9
  )) |>
  rename(prediction = fit, lower = lwr, upper = upr) |> 
    mutate(
    prediction = round(prediction),
    lower = floor(lower),
    upper = ceiling(upper),
    prediction_date = doy_to_date(year, prediction)
  )

predictions_nyc |> 
  kable() |> 
  kable_styling()
```

Again, due to rounding, and because the slope is not very steep, the predictions are the same for every year.

We can check the predictions against the data from previous competition years.

```{r}
#| label: fig-predictions4
#| fig-width: 8
#| fig-height: 3
#| out-width: 100%
#| fig-cap: |
#|   Predictions and 90% prediction intervals from a simple linear regression model for
#|   Washington Square Park in NYC using data from all five sites.
# Plot the predictions alongside the actual observations for 2015 up to 2024.

cherry |>
  right_join(predictions_nyc, by = c("year", "location")) |>
  ggplot(aes(x = year, y = prediction, ymin = lower, ymax = upper)) +
  geom_line(linewidth = 1) +
  geom_ribbon(color = "black", linetype = "22", linewidth = 0.8, fill = NA) +
  geom_point(aes(y = bloom_doy)) +
  scale_x_continuous(breaks = 2019:2024) +
  facet_grid(cols = vars(str_to_title(location))) +
  labs(x = NULL, y = "Peak bloom (days since Jan 1st)")
```

Again, this model does not capture the steepness of the slope of the time trend accurately.

If satisfied with the predictions, we can use them instead of the predictions from before.

```{r}
predictions <- predictions |>
  filter(location != "newyorkcity") |>
  bind_rows(predictions_nyc)
```


## Submission

To submit your entries, enter the predicted bloom dates from your models in the **submission form**.

```{r}
#| label: tbl-05
#| tbl-cap: "Predictions for Submission"

predictions |>
  filter(year == 2025) |>
  mutate(prediction_date = strptime(paste(year, prediction), "%Y %j") |> as_date()) |>
  kable() |> 
  kable_styling()
```

**NOTE**: The question is if we need to round the point estimates or if we can submit partial days.

A simple model may simply take the average maximum winter temperature (Dec. 1st of the previous year until end of February) into account:

```{r}
#| label: create-avg_tmax

avg_winter_temp <- historic_noaa |>
  filter(datatype == "TMAX") |>
  mutate(year = case_when(
    month(date) < 3 ~ year(date),
    month(date) == 12 ~ year(date) + 1,
    TRUE ~ NA_integer_
  )) |>
  filter(!is.na(year), year >= 1970) |>
  group_by(location, year) |>
  summarize(
    avg_tmax = mean(value),
    .groups = "drop"
  )
```

Plot `TMAX_AVG`

```{r}
#| label: fig-avg_tmax
#| fig-cap: "Average maximum winter temperature (Dec. 1st of the previous year until end of February)"

avg_winter_temp |>
  ggplot(aes(x = year, y = avg_tmax)) +
  geom_line() +
  labs(x = NULL, y = "Average maximum winter temperature (°C)") +
  facet_grid(rows = vars(location))
```

Using these average temperature, we can predict the peak bloom date again using linear regression with location-specific temporal trends and a global effect of average winter temperatures. 

```{r}
ls_fit_with_temp <- cherry |>
  inner_join(avg_winter_temp,
    by = c("location", "year")
  ) |>
  lm(formula = bloom_doy ~ year * location + avg_tmax)

cherry_grid <- expand_grid(
  location = unique(cherry$location),
  year = 1990:2025
) |>
  inner_join(avg_winter_temp,
    by = c("location", "year")
  )

predictions_from_temp <- cherry_grid |>
  mutate(pred_bloom = predict(ls_fit_with_temp, newdata = cherry_grid))
```


```{r}
#| label: fig-predictions-avg_tmax
#| fig-cap: "Predictions from Temperature"

predictions_from_temp |>
  left_join(cherry,
    by = c("location", "year")
  ) |>
  ggplot(aes(x = year)) +
  geom_point(aes(y = bloom_doy)) +
  geom_line(aes(y = pred_bloom)) +
  facet_grid(rows = vars(location))
```

The following plot shows a comparison of predictions for Vancouver using the two methods described in this demo.

```{r}
#| label: fig-pedictions-vancouver
#| fig-cap: "Predicted peak bloom (days since Jan 1st) for Vancouver"

predictions_vancouver |>
  left_join(predictions_from_temp,
    by = c("location", "year")
  ) |>
  select(year, pred_temporal = prediction, pred_temp = pred_bloom) |>
  pivot_longer(cols = -year) |>
  mutate(name = if_else(name == "pred_temporal",
    "Method 1: location-based model",
    "Method 2: temperature-based model"
  )) |>
  ggplot() +
  aes(x = year, y = value, linetype = name) +
  geom_line() +
  scale_x_continuous(breaks = 2008:2024) +
  labs(
    x = NULL, linetype = "",
    y = "Predicted peak bloom (days since Jan 1st) for Vancouver"
  ) +
  theme(legend.position = "bottom")
```
